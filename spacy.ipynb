{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"Apple isn't looking at buying U.K. startup for $1 billion\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "n't not PART RB neg x'x False True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 30 34 GPE\n",
      "$1 billion 47 57 MONEY\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "dog True 7.0336733 False\n",
      "cat True 6.6808186 False\n",
      "banana True 6.700014 False\n",
      "afskfsd False 0.0 True\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "tokens = nlp(\"dog cat banana afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "mobile mobile 1.0\n",
      "mobile telecom 0.5194505\n",
      "mobile landline 0.49309456\n",
      "mobile internet 0.50432885\n",
      "mobile mobicom 0.0\n",
      "telecom mobile 0.5194505\n",
      "telecom telecom 1.0\n",
      "telecom landline 0.4987514\n",
      "telecom internet 0.639405\n",
      "telecom mobicom 0.0\n",
      "landline mobile 0.49309456\n",
      "landline telecom 0.4987514\n",
      "landline landline 1.0\n",
      "landline internet 0.5304778\n",
      "landline mobicom 0.0\n",
      "internet mobile 0.50432885\n",
      "internet telecom 0.639405\n",
      "internet landline 0.5304778\n",
      "internet internet 1.0\n",
      "internet mobicom 0.0\n",
      "mobicom mobile 0.0\n",
      "mobicom telecom 0.0\n",
      "mobicom landline 0.0\n",
      "mobicom internet 0.0\n",
      "mobicom mobicom 1.0\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/opt/tljh/user/envs/ds_py37_202006/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  \"\"\"\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "tokens = nlp(\"mobile telecom landline internet mobicom\")\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "3197928453018144401\n",
      "coffee\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\n",
    "print(doc.vocab.strings[3197928453018144401])  # 'coffee'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "I 4690420944186131903 X I I True False True en\n",
      "love 3702023516439754181 xxxx l ove True False False en\n",
      "coffee 3197928453018144401 xxxx c fee True False False en\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]\n",
    "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
    "            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "3197928453018144401\n",
      "coffee\n",
      "coffee\n",
      "coffee\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "doc = nlp(\"I love coffee\")  # Original Doc\n",
    "print(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\n",
    "print(doc.vocab.strings[3197928453018144401])  # 'coffee' üëç\n",
    "\n",
    "empty_doc = Doc(Vocab())  # New Doc with empty Vocab\n",
    "# empty_doc.vocab.strings[3197928453018144401] will raise an error :(\n",
    "\n",
    "empty_doc.vocab.strings.add(\"coffee\")  # Add \"coffee\" and generate hash\n",
    "print(empty_doc.vocab.strings[3197928453018144401])  # 'coffee' üëç\n",
    "\n",
    "new_doc = Doc(doc.vocab)  # Create new doc with first doc's vocab\n",
    "print(new_doc.vocab.strings[3197928453018144401])  # 'coffee' üëç"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\n",
      "Number of entities in KB: 3\n",
      "Number of aliases in KB: 2\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from spacy.kb import KnowledgeBase\n",
    "\n",
    "# load the model and create an empty KB\n",
    "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=3)\n",
    "\n",
    "# adding entities\n",
    "kb.add_entity(entity=\"Q1004791\", freq=6, entity_vector=[0, 3, 5])\n",
    "kb.add_entity(entity=\"Q42\", freq=342, entity_vector=[1, 9, -3])\n",
    "kb.add_entity(entity=\"Q5301561\", freq=12, entity_vector=[-2, 4, 2])\n",
    "\n",
    "# adding aliases\n",
    "kb.add_alias(alias=\"Douglas\", entities=[\"Q1004791\", \"Q42\", \"Q5301561\"], probabilities=[0.6, 0.1, 0.2])\n",
    "kb.add_alias(alias=\"Douglas Adams\", entities=[\"Q42\"], probabilities=[0.9])\n",
    "\n",
    "print()\n",
    "print(\"Number of entities in KB:\",kb.get_size_entities()) # 3\n",
    "print(\"Number of aliases in KB:\", kb.get_size_aliases()) # 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "  Q1004791 0.6000000238418579 [0.0, 3.0, 5.0]\n",
      "  Q42 0.10000000149011612 [1.0, 9.0, -3.0]\n",
      "  Q5301561 0.20000000298023224 [-2.0, 4.0, 2.0]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from spacy.kb import KnowledgeBase\n",
    "\n",
    "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=3)\n",
    "\n",
    "# adding entities\n",
    "kb.add_entity(entity=\"Q1004791\", freq=6, entity_vector=[0, 3, 5])\n",
    "kb.add_entity(entity=\"Q42\", freq=342, entity_vector=[1, 9, -3])\n",
    "kb.add_entity(entity=\"Q5301561\", freq=12, entity_vector=[-2, 4, 2])\n",
    "\n",
    "# adding aliases\n",
    "kb.add_alias(alias=\"Douglas\", entities=[\"Q1004791\", \"Q42\", \"Q5301561\"], probabilities=[0.6, 0.1, 0.2])\n",
    "\n",
    "candidates = kb.get_candidates(\"Douglas\")\n",
    "for c in candidates:\n",
    "    print(\" \", c.entity_, c.prior_prob, c.entity_vector)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['Hello', ',', 'world', '.', 'Here', 'are', 'two', 'sentences', '.']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "doc = nlp(\"Hello, world. Here are two sentences.\")\n",
    "print([t.text for t in doc])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Peach\n",
      "emoji\n",
      "üçë\n",
      "outranking eggplant\n",
      "Peach emoji\n",
      "Peach is the superior emoji.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "doc = nlp(\"Peach emoji is where it has always been. Peach is the superior \"\n",
    "          \"emoji. It's outranking eggplant üçë \")\n",
    "print(doc[0].text)          # 'Peach'\n",
    "print(doc[1].text)          # 'emoji'\n",
    "print(doc[-1].text)         # 'üçë'\n",
    "print(doc[17:19].text)      # 'outranking eggplant'\n",
    "\n",
    "noun_chunks = list(doc.noun_chunks)\n",
    "print(noun_chunks[0].text)  # 'Peach emoji'\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "assert len(sentences) == 3\n",
    "print(sentences[1].text)    # 'Peach is the superior emoji.'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Fine-grained POS tag PROPN 96\n",
      "Coarse-grained POS tag NNP 15794550382381185553\n",
      "Word shape Xxxxx 16072095006890171862\n",
      "Alphabetic characters? True\n",
      "Punctuation mark? False\n",
      "Digit? False\n",
      "Like a number? True\n",
      "Like an email address? False\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "apple = doc[0]\n",
    "print(\"Fine-grained POS tag\", apple.pos_, apple.pos)\n",
    "print(\"Coarse-grained POS tag\", apple.tag_, apple.tag)\n",
    "print(\"Word shape\", apple.shape_, apple.shape)\n",
    "print(\"Alphabetic characters?\", apple.is_alpha)\n",
    "print(\"Punctuation mark?\", apple.is_punct)\n",
    "\n",
    "billion = doc[10]\n",
    "print(\"Digit?\", billion.is_digit)\n",
    "print(\"Like a number?\", billion.like_num)\n",
    "print(\"Like an email address?\", billion.like_email)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "apple <-> banana 0.5800251\n",
      "pasta <-> soup 0.7423402\n",
      "True True True True\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "doc = nlp(\"Mandarin and tangerine are similar. Pasta and soup aren't.\")\n",
    "\n",
    "mandarin = doc[0]\n",
    "tangerine = doc[2]\n",
    "pasta = doc[6]\n",
    "soup = doc[8]\n",
    "\n",
    "print(\"apple <-> banana\", mandarin.similarity(tangerine))\n",
    "print(\"pasta <-> soup\", pasta.similarity(soup))\n",
    "print(apple.has_vector, tangerine.has_vector, pasta.has_vector, soup.has_vector)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc_dep = nlp(\"Visualize a dependency parse and named entities in your browser \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Autonomous cars cars nsubj shift\n",
      "insurance liability liability dobj shift\n",
      "manufacturers manufacturers pobj toward\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ds_py37_202006",
   "language": "python",
   "display_name": "Data Science Py37 202006"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}